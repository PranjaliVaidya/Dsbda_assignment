{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9968c754",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nText Analytics\\n1. Extract Sample document and apply following document preprocessing methods:\\nTokenization, POS Tagging, stop words removal, Stemming and Lemmatization.\\n2. Create representation of document by calculating Term Frequency and Inverse Document\\nFrequency.\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Text Analytics\n",
    "1. Extract Sample document and apply following document preprocessing methods:\n",
    "Tokenization, POS Tagging, stop words removal, Stemming and Lemmatization.\n",
    "2. Create representation of document by calculating Term Frequency and Inverse Document\n",
    "Frequency.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37d17219",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n1) Text Preprocessing:\\n    1. Sentence/word tokenization\\n    2. Removing Stopwords\\n    3. Stemming\\n    4. Lematization\\n    \\n2) TF_IDF\\n    \\n\\n\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "1) Text Preprocessing:\n",
    "    1. Sentence/word tokenization\n",
    "    2. Removing Stopwords\n",
    "    3. Stemming\n",
    "    4. Lematization\n",
    "    \n",
    "2) TF_IDF\n",
    "    \n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13b10510",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194761dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c74df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ff02ab5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4341bdea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this df is not considered in this notebook, but can be used for preprocessing purposes\n",
    "\n",
    "# df = pd.read_csv('https://raw.githubusercontent.com/pycaret/pycaret/master/datasets/amazon.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1fc2c335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fd914d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample text\n",
    "\n",
    "text=\"\"\"Hello Mr. Smith, how are you doing today? The weather is great, and city is awesome.\n",
    "The sky is pinkish-blue. You shouldn't eat cardboard\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "754087ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello Mr. Smith, how are you doing today?', 'The weather is great, and city is awesome.', 'The sky is pinkish-blue.', \"You shouldn't eat cardboard\"]\n"
     ]
    }
   ],
   "source": [
    "tokenized = sent_tokenize(text)\n",
    "print(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ddf4d8f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'Mr.', 'Smith', ',', 'how', 'are', 'you', 'doing', 'today', '?', 'The', 'weather', 'is', 'great', ',', 'and', 'city', 'is', 'awesome', '.', 'The', 'sky', 'is', 'pinkish-blue', '.', 'You', 'should', \"n't\", 'eat', 'cardboard']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "tokenized_words = word_tokenize(text)\n",
    "print(tokenized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7944eaed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d80f6223",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0d8f4cf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'further', 'were', 'in', \"don't\", 'm', 'which', 'an', 'then', 'myself', 's', 'd', \"hasn't\", \"you'd\", 'been', 'both', 'of', 'doesn', 'each', 'other', 'are', \"wasn't\", 'be', 'mightn', 'own', \"mightn't\", 'she', 'hasn', 'until', 'over', 'after', 'they', 'just', 'being', 'now', 'had', \"aren't\", 'most', 'we', 'can', \"you've\", 'doing', 'on', \"haven't\", 'having', 'y', 'is', 'as', \"should've\", 'ma', 'below', 'their', \"shan't\", 't', 'about', 'a', 'off', 'because', 'aren', 'against', 'our', 'them', 'my', \"mustn't\", 'same', 'you', 'yourself', 'from', 'have', \"didn't\", 'once', 'so', 'hadn', 'these', 'up', 'before', 'the', 'me', 'nor', \"needn't\", \"weren't\", \"couldn't\", 'that', 'themselves', 'through', 'only', 'with', 'few', 'needn', 'do', 'hers', 'more', 'out', 'it', 'has', 'for', 'weren', 'itself', 'why', 'down', 'under', 'than', 'very', 'am', 'your', 'this', 'above', 'does', 'into', \"isn't\", 'where', 're', \"she's\", 'its', \"wouldn't\", 've', \"won't\", 'wouldn', 'by', 'no', 'between', 'not', 'isn', 'at', 'yourselves', 'but', 'what', 'did', 'such', 'and', 'who', 'ain', 'ourselves', 'o', 'any', \"it's\", 'or', 'couldn', \"shouldn't\", 'during', 'herself', 'will', \"doesn't\", 'haven', 'ours', 'him', 'himself', 'he', 'wasn', 'don', 'some', 'i', 'shan', 'should', 'shouldn', 'when', 'whom', 'was', \"hadn't\", 'mustn', 'won', \"you're\", 'if', 'while', 'all', 'didn', 'here', 'there', \"you'll\", 'yours', 'those', 'her', 'his', 'll', 'how', 'to', 'too', 'again', \"that'll\", 'theirs'}\n"
     ]
    }
   ],
   "source": [
    "print(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "63152cdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered_Sentence: \n",
      "['Hello', 'Mr.', 'Smith', ',', 'today', '?', 'The', 'weather', 'great', ',', 'city', 'awesome', '.', 'The', 'sky', 'pinkish-blue', '.', 'You', \"n't\", 'eat', 'cardboard']\n"
     ]
    }
   ],
   "source": [
    "filtered_sent = []\n",
    "\n",
    "for w in tokenized_words:\n",
    "    if w not in stopwords:\n",
    "        filtered_sent.append(w)\n",
    "print(\"Filtered_Sentence: \")        \n",
    "print(filtered_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "48036307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed Sentence: ['hello', 'mr.', 'smith', ',', 'today', '?', 'the', 'weather', 'great', ',', 'citi', 'awesom', '.', 'the', 'sky', 'pinkish-blu', '.', 'you', \"n't\", 'eat', 'cardboard']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "stemmed_words=[]\n",
    "for w in filtered_sent:\n",
    "    stemmed_words.append(ps.stem(w))\n",
    "\n",
    "print(\"Stemmed Sentence:\",stemmed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "43d9bad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading wordnet: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "61d3ca24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemmatized Sentence:  ['Hello', 'Mr.', 'Smith', ',', 'today', '?', 'The', 'weather', 'great', ',', 'city', 'awesome', '.', 'The', 'sky', 'pinkish-blue', '.', 'You', \"n't\", 'eat', 'cardboard']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lem = WordNetLemmatizer()\n",
    "\n",
    "lemmatized_words = []\n",
    "\n",
    "for w in filtered_sent:\n",
    "    lemmatized_words.append(lem.lemmatize(w))\n",
    "    \n",
    "print(\"lemmatized Sentence: \", lemmatized_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1cfb2299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: spacy in c:\\users\\samarh\\appdata\\roaming\\python\\python311\\site-packages (3.5.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\samarh\\appdata\\roaming\\python\\python311\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\samarh\\appdata\\roaming\\python\\python311\\site-packages (from spacy) (1.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\samarh\\appdata\\roaming\\python\\python311\\site-packages (from spacy) (1.0.9)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\samarh\\appdata\\roaming\\python\\python311\\site-packages (from spacy) (2.0.7)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\samarh\\appdata\\roaming\\python\\python311\\site-packages (from spacy) (3.0.8)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in c:\\users\\samarh\\appdata\\roaming\\python\\python311\\site-packages (from spacy) (8.1.9)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\samarh\\appdata\\roaming\\python\\python311\\site-packages (from spacy) (1.1.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\samarh\\appdata\\roaming\\python\\python311\\site-packages (from spacy) (2.4.6)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\samarh\\appdata\\roaming\\python\\python311\\site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in c:\\users\\samarh\\appdata\\roaming\\python\\python311\\site-packages (from spacy) (0.7.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in c:\\users\\samarh\\appdata\\roaming\\python\\python311\\site-packages (from spacy) (0.10.1)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\samarh\\appdata\\roaming\\python\\python311\\site-packages (from spacy) (6.3.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\samarh\\appdata\\roaming\\python\\python311\\site-packages (from spacy) (4.65.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\samarh\\appdata\\roaming\\python\\python311\\site-packages (from spacy) (1.24.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\samarh\\appdata\\roaming\\python\\python311\\site-packages (from spacy) (2.28.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in c:\\users\\samarh\\appdata\\roaming\\python\\python311\\site-packages (from spacy) (1.10.7)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\samarh\\appdata\\roaming\\python\\python311\\site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: setuptools in c:\\python311\\lib\\site-packages (from spacy) (65.5.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\samarh\\appdata\\roaming\\python\\python311\\site-packages (from spacy) (23.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\samarh\\appdata\\roaming\\python\\python311\\site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\samarh\\appdata\\roaming\\python\\python311\\site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\samarh\\appdata\\roaming\\python\\python311\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\samarh\\appdata\\roaming\\python\\python311\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\samarh\\appdata\\roaming\\python\\python311\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\samarh\\appdata\\roaming\\python\\python311\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.12.7)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\samarh\\appdata\\roaming\\python\\python311\\site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\samarh\\appdata\\roaming\\python\\python311\\site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\samarh\\appdata\\roaming\\python\\python311\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\samarh\\appdata\\roaming\\python\\python311\\site-packages (from typer<0.8.0,>=0.3.0->spacy) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\samarh\\appdata\\roaming\\python\\python311\\site-packages (from jinja2->spacy) (2.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2f4fb3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "680ac54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "39a55a31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello | INTJ | interjection | UH interjection\n",
      "\n",
      "Mr. | PROPN | proper noun | NNP noun, proper singular\n",
      "\n",
      "Smith | PROPN | proper noun | NNP noun, proper singular\n",
      "\n",
      ", | PUNCT | punctuation | , punctuation mark, comma\n",
      "\n",
      "how | SCONJ | subordinating conjunction | WRB wh-adverb\n",
      "\n",
      "are | AUX | auxiliary | VBP verb, non-3rd person singular present\n",
      "\n",
      "you | PRON | pronoun | PRP pronoun, personal\n",
      "\n",
      "doing | VERB | verb | VBG verb, gerund or present participle\n",
      "\n",
      "today | NOUN | noun | NN noun, singular or mass\n",
      "\n",
      "? | PUNCT | punctuation | . punctuation mark, sentence closer\n",
      "\n",
      "The | DET | determiner | DT determiner\n",
      "\n",
      "weather | NOUN | noun | NN noun, singular or mass\n",
      "\n",
      "is | AUX | auxiliary | VBZ verb, 3rd person singular present\n",
      "\n",
      "great | ADJ | adjective | JJ adjective (English), other noun-modifier (Chinese)\n",
      "\n",
      ", | PUNCT | punctuation | , punctuation mark, comma\n",
      "\n",
      "and | CCONJ | coordinating conjunction | CC conjunction, coordinating\n",
      "\n",
      "city | NOUN | noun | NN noun, singular or mass\n",
      "\n",
      "is | AUX | auxiliary | VBZ verb, 3rd person singular present\n",
      "\n",
      "awesome | ADJ | adjective | JJ adjective (English), other noun-modifier (Chinese)\n",
      "\n",
      ". | PUNCT | punctuation | . punctuation mark, sentence closer\n",
      "\n",
      "\n",
      " | SPACE | space | _SP whitespace\n",
      "\n",
      "The | DET | determiner | DT determiner\n",
      "\n",
      "sky | NOUN | noun | NN noun, singular or mass\n",
      "\n",
      "is | AUX | auxiliary | VBZ verb, 3rd person singular present\n",
      "\n",
      "pinkish | NOUN | noun | NN noun, singular or mass\n",
      "\n",
      "- | PUNCT | punctuation | HYPH punctuation mark, hyphen\n",
      "\n",
      "blue | ADJ | adjective | JJ adjective (English), other noun-modifier (Chinese)\n",
      "\n",
      ". | PUNCT | punctuation | . punctuation mark, sentence closer\n",
      "\n",
      "You | PRON | pronoun | PRP pronoun, personal\n",
      "\n",
      "should | AUX | auxiliary | MD verb, modal auxiliary\n",
      "\n",
      "n't | PART | particle | RB adverb\n",
      "\n",
      "eat | VERB | verb | VB verb, base form\n",
      "\n",
      "cardboard | NOUN | noun | NN noun, singular or mass\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token, \"|\", token.pos_,\"|\", spacy.explain(token.pos_),\"|\",token.tag_, spacy.explain(token.tag_))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5bc98c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required module\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b7278074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create object\n",
    "tfidf = TfidfVectorizer()\n",
    " \n",
    "# get tf-df values\n",
    "result = tfidf.fit_transform(lemmatized_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "64dba772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "idf values:\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'TfidfVectorizer' object has no attribute 'get_feature_names'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TfidfVectorizer\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124midf values:\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ele1, ele2 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[43mtfidf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_feature_names\u001b[49m(), tfidf\u001b[38;5;241m.\u001b[39midf_):\n\u001b[0;32m      5\u001b[0m \t\u001b[38;5;28mprint\u001b[39m(ele1, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m'\u001b[39m, ele2)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'TfidfVectorizer' object has no attribute 'get_feature_names'"
     ]
    }
   ],
   "source": [
    "# get idf values\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "print('\\nidf values:')\n",
    "for ele1, ele2 in zip(tfidf.get_feature_names(), tfidf.idf_):\n",
    "\tprint(ele1, ':', ele2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f52a80df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word indexes:\n",
      "{'hello': 6, 'mr': 7, 'smith': 10, 'today': 12, 'the': 11, 'weather': 13, 'great': 5, 'city': 3, 'awesome': 0, 'sky': 9, 'pinkish': 8, 'blue': 1, 'you': 14, 'eat': 4, 'cardboard': 2}\n",
      "\n",
      "tf-idf value:\n",
      "  (0, 6)\t1.0\n",
      "  (1, 7)\t1.0\n",
      "  (2, 10)\t1.0\n",
      "  (4, 12)\t1.0\n",
      "  (6, 11)\t1.0\n",
      "  (7, 13)\t1.0\n",
      "  (8, 5)\t1.0\n",
      "  (10, 3)\t1.0\n",
      "  (11, 0)\t1.0\n",
      "  (13, 11)\t1.0\n",
      "  (14, 9)\t1.0\n",
      "  (15, 1)\t0.7071067811865475\n",
      "  (15, 8)\t0.7071067811865475\n",
      "  (17, 14)\t1.0\n",
      "  (19, 4)\t1.0\n",
      "  (20, 2)\t1.0\n",
      "\n",
      "tf-idf values in matrix form:\n",
      "[[0.         0.         0.         0.         0.         0.\n",
      "  1.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         1.         0.         0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         1.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  1.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         1.\n",
      "  0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         1.         0.        ]\n",
      " [0.         0.         0.         0.         0.         1.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.         0.         0.         1.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [1.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         1.\n",
      "  0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         1.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.         0.70710678 0.         0.         0.         0.\n",
      "  0.         0.         0.70710678 0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         1.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         1.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.         0.         1.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# get indexing\n",
    "print('\\nWord indexes:')\n",
    "print(tfidf.vocabulary_)\n",
    "\n",
    "# display tf-idf values\n",
    "print('\\ntf-idf value:')\n",
    "print(result)\n",
    "\n",
    "# in matrix form\n",
    "print('\\ntf-idf values in matrix form:')\n",
    "print(result.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c3ea7479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Document:\n",
      " This is a sample document for preprocessing. It includes tokenization, POS tagging, stop words removal, stemming, and lemmatization.\n",
      "\n",
      "Tokenization:\n",
      " ['This', 'is', 'a', 'sample', 'document', 'for', 'preprocessing', '.', 'It', 'includes', 'tokenization', ',', 'POS', 'tagging', ',', 'stop', 'words', 'removal', ',', 'stemming', ',', 'and', 'lemmatization', '.']\n",
      "\n",
      "POS Tagging:\n",
      " [('This', 'DT'), ('is', 'VBZ'), ('a', 'DT'), ('sample', 'JJ'), ('document', 'NN'), ('for', 'IN'), ('preprocessing', 'VBG'), ('.', '.'), ('It', 'PRP'), ('includes', 'VBZ'), ('tokenization', 'NN'), (',', ','), ('POS', 'NNP'), ('tagging', 'NN'), (',', ','), ('stop', 'VB'), ('words', 'NNS'), ('removal', 'JJ'), (',', ','), ('stemming', 'VBG'), (',', ','), ('and', 'CC'), ('lemmatization', 'NN'), ('.', '.')]\n",
      "\n",
      "Stop Words Removal:\n",
      " ['sample', 'document', 'preprocessing', '.', 'includes', 'tokenization', ',', 'POS', 'tagging', ',', 'stop', 'words', 'removal', ',', 'stemming', ',', 'lemmatization', '.']\n",
      "\n",
      "Stemming:\n",
      " ['sampl', 'document', 'preprocess', '.', 'includ', 'token', ',', 'po', 'tag', ',', 'stop', 'word', 'remov', ',', 'stem', ',', 'lemmat', '.']\n",
      "\n",
      "Lemmatization:\n",
      " ['sample', 'document', 'preprocessing', '.', 'includes', 'tokenization', ',', 'POS', 'tagging', ',', 'stop', 'word', 'removal', ',', 'stemming', ',', 'lemmatization', '.']\n",
      "\n",
      "TF-IDF Representation:\n",
      "lemmatization : 0.24253562503633297\n",
      "and : 0.24253562503633297\n",
      "stemming : 0.24253562503633297\n",
      "removal : 0.24253562503633297\n",
      "words : 0.24253562503633297\n",
      "stop : 0.24253562503633297\n",
      "tagging : 0.24253562503633297\n",
      "pos : 0.24253562503633297\n",
      "tokenization : 0.24253562503633297\n",
      "includes : 0.24253562503633297\n",
      "it : 0.24253562503633297\n",
      "preprocessing : 0.24253562503633297\n",
      "for : 0.24253562503633297\n",
      "document : 0.24253562503633297\n",
      "sample : 0.24253562503633297\n",
      "is : 0.24253562503633297\n",
      "this : 0.24253562503633297\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Sample document\n",
    "document = \"This is a sample document for preprocessing. It includes tokenization, POS tagging, stop words removal, stemming, and lemmatization.\"\n",
    "\n",
    "# Tokenization\n",
    "tokens = word_tokenize(document)\n",
    "\n",
    "# POS Tagging\n",
    "pos_tags = pos_tag(tokens)\n",
    "\n",
    "# Stop words removal\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "\n",
    "# Stemming\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]\n",
    "\n",
    "# Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
    "\n",
    "# Print the results\n",
    "print(\"Original Document:\\n\", document)\n",
    "print(\"\\nTokenization:\\n\", tokens)\n",
    "print(\"\\nPOS Tagging:\\n\", pos_tags)\n",
    "print(\"\\nStop Words Removal:\\n\", filtered_tokens)\n",
    "print(\"\\nStemming:\\n\", stemmed_tokens)\n",
    "print(\"\\nLemmatization:\\n\", lemmatized_tokens)\n",
    "\n",
    "# Calculate Term Frequency-Inverse Document Frequency (TF-IDF)\n",
    "corpus = [document]\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Print the TF-IDF representation\n",
    "print(\"\\nTF-IDF Representation:\")\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "for col in tfidf_matrix.nonzero()[1]:\n",
    "    print(feature_names[col], \":\", tfidf_matrix[0, col])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb2fbe3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14cfa55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c0fad4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
